package com.spark.batch.example.ReadWriteAvroFile

import com.spark.batch.example.AppConfig._
import com.spark.batch.example.Utils
import com.spark.batch.example.Utils.{conf, moduleRootDir}

/**
 * The idea of AppOne are as follows:
 * 1. Read com.spark.stream.example.avro file generated by kafka producer using spark
 * 2. Write the result of step 1 to com.spark.stream.example.avro using spark
 * 3. Read the output of step 2  to check if write was fine
 *
 * com.spark.batch.example.ReadWriteAvroFile.AppOneWriter reads com.spark.stream.example.avro file and applies schema from asvc file and create a dataframe
 * The same dataframe is then written by spark in com.spark.stream.example.avro format
 */

object AppOneWriter extends App {
  val avroDataFilePath = moduleRootDir + conf.getString(REVIEW_SOURCE_DATA_FILE_PATH)
  val avroSchemaFilePath = moduleRootDir + conf.getString(REVIEW_SOURCE_SCHEMA_FILE_PATH)
  val outputPath = conf.getString(REVIEW_TARGET_DATA_FILE_PATH) + "AppOneWriter"
  println(outputPath)

  //Schema from Schema File- reviewsV1.avsc
  val schemaAvro = Utils.getSchemaFromAvroFile(avroSchemaFilePath)

  val valueDF = Utils.createDFFromAvroFile(avroDataFilePath, schemaAvro)
  valueDF.show(false)

  valueDF.coalesce(1).write.format("com.spark.stream.example.avro").mode("overwrite").save(outputPath)
}

/**
 * com.spark.batch.example.ReadWriteAvroFile.AppOneReaderOne reads com.spark.stream.example.avro file written by spark
 * and applies schema from asvc file(reviewsV1.avsc) and create a dataframe
 */
object AppOneReaderOne extends App {

  val sourcePath =  conf.getString(REVIEW_TARGET_DATA_FILE_PATH) + "AppOneWriter"
  val avroSchemaFilePath = moduleRootDir + conf.getString(REVIEW_SOURCE_SCHEMA_FILE_PATH)

  //Schema from Schema File- reviewsV1.avsc
  val schemaAvro = Utils.getSchemaFromAvroFile(avroSchemaFilePath)
  val df=  Utils.createDFFromAvroFile(sourcePath,schemaAvro)
  df.show(false)
}

/**
 * com.spark.batch.example.ReadWriteAvroFile.AppOneReaderTwo reads com.spark.stream.example.avro file written by spark
 * and applies schema from schema Registry and create a dataframe
 *
 * The schema of schema registry is published by kafka producer in other application
 */

object AppOneReaderTwo extends App {

  val sourcePath =  conf.getString(REVIEW_TARGET_DATA_FILE_PATH) + "AppOneWriter"

  //Schema from Schema Registry. Same data was pushed to kafka topic with same schema
  val schemaAvro = Utils.getSchemaFromAvroSchemaRegistry(conf.getString(REVIEW_KAFKA_SOURCE_TOPIC))
  val df=  Utils.createDFFromAvroFile(sourcePath,schemaAvro)
  df.show(false)
}

/**
 * The idea of AppTwo are as follows:
 * 1. Read com.spark.stream.example.avro file generated by kafka producer using spark
 * 2. Do basic transformation like aggregation
 * 3. Write the result of step 2 to com.spark.stream.example.avro using spark
 * 4. Create schema file(review_transformed_schemaV1.avsc) with expected columns from Step 2
 * 5. Read the output of step 3 and schema file from step 4 using spark to check if write was fine
 * 6. Register the schema of step 4 with Schema registry
 * 7. Again read the data written in step 3 but this time use schema from schema Registry
 *
 * com.spark.batch.example.ReadWriteAvroFile.AppTwoWriter reads com.spark.stream.example.avro file and applies schema from asvc file.
 * It does some aggregation and generate 2 columns
 * The transformedDF dataframe is then written by spark in com.spark.stream.example.avro format
 */


object AppTwoWriter extends App{

  val avroDataFilePath = moduleRootDir + conf.getString(REVIEW_SOURCE_DATA_FILE_PATH)
  val avroSchemaFilePath = moduleRootDir + conf.getString(REVIEW_SOURCE_SCHEMA_FILE_PATH)
  val outputPath = conf.getString(REVIEW_TARGET_DATA_FILE_PATH) + "AppTwoWriter"

  //Schema from Schema File- reviewsV1.avsc
  val schemaAvro = Utils.getSchemaFromAvroFile(avroSchemaFilePath)

  val valueDF = Utils.createDFFromAvroFile(avroDataFilePath, schemaAvro)
  //valueDF.show(false)

  import org.apache.spark.sql.functions._

  val transformedDF=valueDF
    .groupBy("marketplace").agg(sum(col("market_price")-col("sale_price")).as("profit"))

  transformedDF.printSchema()
  transformedDF.show(false)

  transformedDF.coalesce(1).write.format("com.spark.stream.example.avro").mode("overwrite").save(outputPath)
}

/**
 * com.spark.batch.example.ReadWriteAvroFile.AppTwoReaderOne reads com.spark.stream.example.avro file written by com.spark.batch.example.ReadWriteAvroFile.AppTwoWriter using the manually
 * created schema file(review_transformed_schemaV1.avsc)
 */
object AppTwoReaderOne extends App {

  val sourcePath =  conf.getString(REVIEW_TARGET_DATA_FILE_PATH) + "AppTwoWriter"
  val avroSchemaFilePath = moduleRootDir + conf.getString(REVIEW_TARGET_SCHEMA_FILE_PATH)

  //Schema from Schema File- review_transformed_schemaV1.avsc
  val schemaAvro = Utils.getSchemaFromAvroFile(avroSchemaFilePath)
  val df=  Utils.createDFFromAvroFile(sourcePath,schemaAvro)
  df.show(false)
}

/**
 * com.spark.batch.example.ReadWriteAvroFile.RegisterSchemaOnSchemaRegistryForAppTwo register the manually
 * created schema file(review_transformed_schemaV1.avsc) with schema registry
 */

object RegisterSchemaOnSchemaRegistryForAppTwo extends App {

  import io.confluent.kafka.schemaregistry.client.rest.RestService

  val schemaRegistryUrl = conf.getString(REVIEW_SCHEMA_REGISTRY_URL)
  val schemaSubject = conf.getString(REVIEW_KAFKA_TARGET_TOPIC) + "-value"

  // Create a new RestService instance
  val restService = new RestService(schemaRegistryUrl)

  // Parse the Avro schema from the schema string
  val avroSchemaFilePath = moduleRootDir + conf.getString(REVIEW_TARGET_SCHEMA_FILE_PATH)
  val avroSchema = Utils.getSchemaFromAvroFile(avroSchemaFilePath)

  // Register the schema with the Schema Registry
  val schemaId = restService.registerSchema(avroSchema, schemaSubject)

  // Print the ID of the registered schema
  println(s"Registered schema with ID: $schemaId")
}

/**
 * com.spark.batch.example.ReadWriteAvroFile.AppTwoReaderTwo reads com.spark.stream.example.avro file written by com.spark.batch.example.ReadWriteAvroFile.AppTwoWriter using the schema from
 * Schema registry generated using com.spark.batch.example.ReadWriteAvroFile.RegisterSchemaOnSchemaRegistryForAppTwo
 */

object AppTwoReaderTwo extends App{
  val sourcePath =  conf.getString(REVIEW_TARGET_DATA_FILE_PATH) + "AppTwoWriter"
  val schemaAvro = Utils.getSchemaFromAvroSchemaRegistry(conf.getString(REVIEW_KAFKA_TARGET_TOPIC))

  val valueDF = Utils.createDFFromAvroFile(sourcePath, schemaAvro)
  valueDF.printSchema()
  valueDF.show(false)

}